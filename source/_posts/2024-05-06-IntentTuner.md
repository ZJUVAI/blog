---
title: "IntentTuner: An Interactive Framework for Integrating Human Intentions in Fine-tuning Text-to-Image Generative Models"
tags: ["论文评述", "报告"]
date: 2024-05-06
author: 王逸瑶
mathjax: true
---

论文：IntentTuner: An Interactive Framework for Integrating Human Intentions in Fine-tuning Text-to-Image Generative Models

作者：Xingchen Zeng, Ziyao Gao, Yilin Ye, Wei Zeng

发表：CHI 2024

微调有助于帮助文生图模型理解新的概念（例如，风格和肖像），使用户能够创造性地定制内容。最近关于微调的研究主要集中在减少训练数据和减轻计算负载，但忽视了与用户意图的对齐，特别是在手动策划多模态训练数据和面向意图的评估方面。通过与微调实践者进行的形成性研究，以理解用户意图，我们提出了IntentTuner，一个在微调工作流的每个阶段都智能地融入人类意图的交互式框架。IntentTuner使用户能够用图像示例和文本描述来表达训练意图，自动将它们转化为有效的数据增强策略。此外，IntentTuner引入了新的指标来衡量用户意图对齐，允许对模型训练进行意图感知的监控和评估。应用示例和用户研究表明，IntentTuner简化了微调，减少了认知负担，并与常见的基线工具相比有很大提升。

[论文链接](https://doi.org/10.1145/3613904.3642165)

## 研究背景
在文生图模型的使用中，如果直接使用预训练模型，可能会出现结果不符合用户预期需求的情况，特别是在训练数据中未包含用户所需概念的时候。

- 生成结果与用户的表达相悖
- 生成结果的设计不符合用户所需

比如下图，这个模型的训练数据中没有出现过小黄人的相关数据，它就会无法理解用户给出的画一个小黄人的指令代表什么。
![image.png](image-1.png)
一种常见的解决方法是**微调：**让预训练模型额外训练少量和用户的需求相关的样本来学习新的概念。现有的研究主要侧重于开发高效的微调方法，目标是减少模型学习新概念所需的图像数量，以及减轻计算资源需求。
然而，选择了特定的微调方法后，有效地应用微调后的结果仍然具有挑战性。在微调时，用户通常会有概念性的意图，如下图所示，用户可能会指定模型应该学习和保持什么特征，什么特征应该被修改或删除。
当进一步将这些意图与微调过程中涉及的技术步骤对齐时，会出现困难。具体来说，用户需要将他们的意图转化为具体的数据策略（例如，图像增强和标题优化）和性能评估（例如，模型评估和选择），这目前是一个试错过程。
![image.png](image-2.png)
比如用户要生成小黄人，就会对生成的小黄人有一定的要求和限制，比如希望小黄人是一只眼睛或者两只眼睛的，而不是像下图里是三个或者四个眼睛。以及可能对小黄人的服装有要求，希望在微调后依然要保持是蓝色背带裤。
但是通过传统的微调方法，只能通过给他少量符合用户需求的图片来给模型做微调，并希望模型能够通过图片来理解小黄人的形象，而无法对形象、穿着有硬性的要求。
![image.png](image-3.png)
这就导致以下几点挑战：

- 微调之后的模型不能很好的契合用户的意图
- 评估生成图像的质量/微调模型和用户意图的契合度存在困难
- 现有微调可视化工具（Koyhass等）只支持低级参数的交互微调
## 工作以及贡献
为了解决这些挑战，作者提出了IntentTuner，一个将用户意图集成到文生图模型微调过程中的交互式框架。
总结贡献如下:

- 引入了一个新的交互框架，通过将用户输入分解为与意图对齐的数据处理、模型监控和评估策略三个部分，智能地将人类意图集成到微调文本到图像生成模型中。
- 开发了一个集成的系统，将微调和生成统一到一个整体接口中，并支持多模态输入，使专家和新手用户都能自然的表达自己的意图，从而灵活地微调文生图模型。



## 预先实验（Preliminary Study）
在开发前先进行了一个实验，来发现当前微调文生图模型的流程和痛点。
### 参与人员
作者找了四位对微调有一定了解的不同行业的参与者，

- 一位热衷于模型训练的电子商务专业人士（男，年龄：23）
   - 他在网上微调和分享模型，但对微调的技术原理了解有限，并且难以持续获取高质量的数据集和模型；
- 两位计算机科学和工业设计专业的中级模型训练者（男，年龄：23-27）
   - 他们为学术研究微调模型，并研究了各种设置对训练结果的影响，并在特定领域拥有高质量的数据集和模型；
- 一位具有插图背景的专业模型训练者（女，年龄：25）
   - 她为商业用途微调模型，并对结果的稳定性和可控性有更高的要求，对模型微调的技术原理有全面的理解。
### 实验任务
任务1：参与者被要求使用他们已经准备好的数据集，在他们熟悉的领域中进行完整的微调过程
任务2：他们被分配去在一个不熟悉的领域微调一个模型。
### 实验总结
在完成实验后，作者总结了他们的发现。
#### 通用工作流程
作者首先总结了微调过程中的通用工作流程，如下图所示
![image.png](image-4.png)

- **数据预处理：**在这个阶段，用户分类、裁剪和标记原始数据集，以增强与意图对齐的视觉特征，同时减少不需要的内容。例如，用户在微调二维角色模型时，手动复制了原始图像，然后裁剪了衣服，他的意图是“强调角色服装特征”。至于标记（tag）可以使用自动化工具来完成，但是需要进行手动修正，因为标记内容需要与用户意图对齐，而当前的解决方案无法完成这一点。
- **模型训练：**基于处理过的数据，用户设置训练参数，开始训练过程。如果出现异常（例如，损失值不收敛），该过程将被终止，用户将调整数据集或超参数以重新开始训练。但是，训练过程并不透明，因为用户只能通过日志数据（如损失值）来监控进度，这些数据只能表明训练是否收敛。由于图像生成质量匹配意图是非常主观的，无法用损失值衡量，一些用户选择定期的生成样本图像查看。
- **模型评估：**用户从训练过程中获得的一系列检查点中选择最好的模型。但有些检查点往往不能令人满意，用户需要根据多种评估指标手动调整微调方案，从而更好地匹配用户的意图。
#### 用户意图
之后，作者总结了用户在微调中的意图，作者将用户意图总结为领域、概念和操作三部分，如下图所示。
![image.png](image-5.png)

- **领域**指的是微调目标的专业创作领域（例如，2D角色、人像、产品设计和绘画）。基于目标领域的一般特性，用户可以对训练的重点形成初步的理解。比如对于一个卡通角色最主要的是它的着装和长相，对于产品设计来说，最重要的可能是产品的整体风格；
- **概念**是意图中指定的元素，如发型长度、服装和背景。根据概念的范围可以把概念分成三类：（看图说）发色、面部表情这种属于属性级别的概念，头发、脸这些属于实例级别的概念，背景、照明氛围这种属于图像级别的概念；
- **操作**指的是在生成结果中对概念的预期操作，可以分为保留、修改和删除。比如对于小黄人这个卡通角色，我想保留现在的蓝色背带裤的设计，但希望修改小黄人的发型和表情，最后我希望删掉戴帽子的小黄人和穿裙子的小黄人。
### 挑战
通过Preliminary Study，作者总结挑战如下：

- C1: 将抽象的意图转化为明确的数据策略是具有挑战性的。  
- C2: 视觉样本的不足和不合理的文本标题阻碍了意图的对齐。  
- C2.1: 视觉样本不足。  
- C2.2: 自动标记的文本结果不可靠，而手动标记很繁琐。  
- C3: 缺少直观的训练监控和有效的评估。  
- C3.1: 对黑盒训练过程的监控不直观。  
- C3.2: 缺乏与意图对齐的评估指标。
### 设计目标
对应挑战，作者总结设计目标如下：

- G1: 通过自然描述和交互理解用户意图。  
- G2: 提供高效的与意图对齐的数据增强。  
- G3: 提供意图感知的直观监控和模型性能评估。
## 工作流程（approach）

- General pipeline（左）：依赖试错过程来检查系统是否正确理解了他们的意图，且手动预处理训练图像，如裁剪、分类和标记，并观察生成的图像。
- Our pipeline（右）：通过用户表达的意图自动引导微调中的重要节点：数据增强、模型训练和评估

![image.png](image-6.png)
### 语言-视觉意图对齐和转换（Language-Vision Intent Alignment and Transformation）
当用户打算在保持他的衣服的同时教模型一个新的人类概念时，他们可能最初用简单的关键词描述他们的意图（例如，“学习黑色夹克”）。语言-视觉模型无法完全澄清用户的意图，因为有两种特定类型的黑夹克，即黑色皮夹克和黑色条纹夹克。传达这种详细的意图需要更具体的文本描述，而一般的视觉模型仍然难以直接区分这种细粒度的类别。
用户可以通过提供文本描述和参考图像来表达他们的意图。具体来说，用户可以从他们的图像集中选择参考图像，并使用边界框来选择特定的视觉概念。作者设定了一种独特的语法，帮助用户在文本中引用这些视觉概念，用括号中的数字表示（例如，“[1]”）。例如，用户输入："我想训练一个名叫Vincent的男人的模型。确保他的面部特征保持一致。他应该能够在黑色皮夹克[1]和黑色条纹夹克[2]之间切换。他的发色应该可以调整，不让他戴项链。"最终的目标是提取一个具体的意图层次结构。
![image.png](image-7.png)
### 数据增强（Intent-guided Data Augmentation）

#### (1) 图像增强
如下图所示，图像增强包括两个主要阶段：
1）检测和过滤与意图相关的概念；
2）基于不同操作的增强。
首先，使用GroundingDino，这是一个擅长识别与文本描述对应的图像区域的预训练视觉模型。如图所示，GroundingDino将与意图相关的概念作为文本输入并检测它们相关的视觉元素的边界框。为了解决GroundingDino检测中固有的语义模糊性，作者利用之前用户给的参考图像作为一个“过滤器”，也就是计算检测到的概念与参考图像之间的相似性，并过滤掉相似性低的视觉概念。这种方法有效地降低了具有相似语义含义的错误视觉概念被检测的可能。基于检测到的边界框，会根据不同的意图执行不同的增强策略。  
**删除意图**将需要进一步的图像修复。直接从视觉概念的边界框中裁剪图像是一种比较直观的方法。然而，这种方法可能也会移除原始图像裁剪区域中嵌入的视觉和语义信息。为了减轻这个问题，作者采用图像修复技术[44]来移除视觉概念，同时根据周围区域重绘被移除的区域，从而尽可能保留原始的语义和视觉信息。  
**保留意图**将被转译为在边界框内裁剪图像，并将裁剪部分作为训练数据集中的独立图像添加。这将使模型在后续的训练阶段更加关注概念的语义和视觉信息。  
**修改意图**与保留类似。不同的是，作者为图像裁剪操作设置了一个触发阈值（也就是，概念在原始图像中的面积百分比，默认设置为40%，如果小于40%就不触发）。首先，过多的新图像会显著增加训练时间。其次，重复原始图像中本身显著的概念容易导致过拟合。  
处理后的图像将构成一个由多个子文件夹（如图中的脸、头发和全身）组成的新图像数据集，这个数据集会用于后续的训练。
![image.png](image-8.png)
#### (2) 标题优化
可以直接用clip等工具对这些图像写文字来描述，但是这样就无法与用户的意图对齐。
所以作者设计了智能与意图对齐的标题优化方法。流程如下图所示：首先利用BLIP2自动生成图像的初始标题，并在其开头添加用户指定的触发词（比如图a中，用户指定来一个superhero landing）的触发词。接下来，再使用标题优化方法帮助用户更清楚的描述其保留和修改操作。对于图a中的superhero landing，如果要模型能够学习到superhero landing和这个图片中人的姿势的相关性，那描述的语言里就不可以有与这个动作相关的描述词，否则模型会无法将superhero landing和图片清楚的关联起来。blip生成的这段话中标红的部分，就是对这个动作的描述，所以需要移除（文中图片有误，少移除一句话）
对于修改操作，这个操作的目的是控制和改变模型中的某些概念，这样我们就不能将视觉特征直接和触发词或标题绑定。因此，和触发词相反，应该提供详细的描述，但是也不能缺少关键特征的文字描述。比如图b，全身的图失去了头发的描述，因为头发占图片比例比较小，但之前我们用检测和切割的方法截出了头发部分的图片，因此我们有两个不同的描述，最后让LLM合并这两个描述，就得到最终的标题。
![image.png](image-9.png)
### 与意图对齐的生成图像评估（Intent-aligned Image Evaluation）
**目标：**量化评估模型输出与用户意图的对齐程度
通过稳定性和可控性两个方面评估文本到图像微调的效果

- 稳定性：测量模型复制目标概念的能力，防止过拟合
- 可控性：测量模型使用文本提示修改概念的能力，例如，头发长度的可控性



#### 稳定性评估

- 在用户期望的概念粒度上进行，允许用户独立评估多个意图
- 裁剪与意图相关的对象，计算它们与每个生成图像的视觉相似性
- 利用人类偏好分类器缓解传统方法忽略人类偏好感知的问题

#### 可控性评估

- 使用包含指定概念相反属性的提示生成采样图像  
- 在高维非线性神经嵌入空间（如CLIP）测量标准的语义对齐度量
- 利用两个相反的属性关键词将图像评估任务转换为二元分类，降低单一属性带来的歧义
- 计算每个图像与用户预期控制属性的两个相反文本关键词在潜在空间的余弦相似度

## 系统设计
A 意图-数据对齐模块
B 训练监视器模块
C 模型评估模块
![image.png](image-10.png)

## 评估
作者先在两个场景下对本框架做了评估，再做了用户实验
### 使用场景
下图是一个抽象概念的使用场景。这里给了一个抽象的概念“Superhero landing”，用intent tuner调试的结果对这一动作的理解更好
![image.png](image-11.png)
下图是一个多概念的使用场景，即模型学习多个不同概念并进行区分。同样，intent tuner的微调表现更好，对各个概念能有更好的区分
![image.png](image-12.png)
### 用户实验
如下图所示，intent tuner相比于baseline有了很大的提升。
![image.png](image-13.png)
![image.png](image-14.png)
