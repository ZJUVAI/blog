# R-Tuning Instructing Large Language Models to Say 'I Don't Know' 论文解读

---
title: "R-Tuning Instructing Large Language Models to Say ‘I Don’t Know'"
tags: ["论文评述", "报告"]
date: 2024-05-27
author: 杨兆瑞
mail: zhaorui.yang@zju.edu.cn
mathjax: true

---

## 引言

- LLM倾向于捏造事实以回答知识边界外的问题
- 其原因是LLM内部参数中包含的知识和用于指令微调的数据之间存在gap
- 分析：
  - 模型的几乎所有知识在预训练阶段习得
  - 指令微调使模型学会格式
  - 指令微调强迫模型补全句子，即使在面对知识边界外的问题时


## 动机

在现有的LLM指令微调过程中，训练数据仅包含确切回复。在面对不知道的问题时，LLM不会回复"I don't know"，而是通过猜测完成回答。Motivated by this，作者提出在知识的边界处进行微调，让LLM表达自信程度，并在面对未知问题时拒绝回答。

![image.png](./1717554249556-f01ec954-2746-43fa-9517-fffb23ccbec6.png)

## 方法

方法名为**R**efusal-Aware Instruction **Tuning** (R-Tuning)，其包含两个主要步骤：

1. 衡量知识边界，识别出LLM不确定的问题：通过一次推理，根据回答和标签对训练数据进行二分类：Uncertain data $D_0$和 Certain data $D_1$
2. 通过在标签词后添加表示确信度的单词，构造 refusal-aware data，并在构建的数据集上微调。



下图是方法示例

![image.png](./1717554455228-40cde7c8-49ec-460b-819a-41a6534edd2f.png)

## 数据构建

有一个模板：

```
Q : {Question}, A : {Answer}. {Prompt}
Prompt: Are you sure you accurately answered the question based on your internal knowledge?
```

根据回答与标签是否匹配，在后面添加两种padding：

- Certain: I am sure
- Uncertain: I am unsure

## 训练

- 监督微调（Supervised Fine-Tuning）
- 仅仅对上文的uncertainty part（即padding）计算loss

![image.png](./1717554633433-ea6cc197-5e39-4d40-83d9-25b5e7715951.png)

## 推理

首先，将问题填入以下模板中：

```
Q: {Question}, A:
```

然后在后面添加prompt：*Are you sure you accurately answered the question based on your internal knowledge? I am*

## 实验

### 设定

#### 数据集

- Question Answering

  - ParaRel

  - HotpotQA

  - SelfAware

  - HaluEval

  - FalseQA

  - NEC


- Multiple Choice

  - MMLU

  - WiCE

  - FEVER

#### 实验任务

- 单任务（Single-task）
  - 在ParaRel 和 MMLU数据集上进行
  - 将数据集手工划分成三部分：
    - Training set
    - In-domain test set
    - Out-of-domain test set

- 多任务（Multi-task）
  - 将ParaRel, MMLU, WiCE, HotpotQA 和 FEVER 数据集混合
  - 评估时，用每个数据集相应的测试集作为In-domain test set，用HaluEval 数据集作为OOD test set


### 基线方法

- Pretrain-T：使用原始模型在全部测试集上测试
- Pretrain-W：使用原始模型在微调后模型仍然给出答复的子集上测试
- Vanilla：使用传统微调方法，用问题和标签对原始模型进行微调后获得的模型

### 评估

![image.png](./1717555114372-57fe384c-92a2-4fe1-9f8a-b02b9f8ee195.png)

![image.png](./1717555117684-796bc65b-022e-48e2-bf52-ad106aec21a4.png)

![image.png](./1717555120290-d8d1f288-bab6-442a-8f44-03d17b8b5941.png)

一个理想模型会对自己给出的正确答案有high confidence，而对幻觉有low confidence，从而带来高AP值。如果模型对所有答案都有high confidence，则会带来较低的AP值，从而说明AP这个指标的合理性。

### 实现细节

- 模型：OpenLLaMA-3B，LLaMA-7B 和 LLaMA-13B
- 超参数：

- 1 epoch
- lr: 2e-5
- batch_size: 4
- A100-40GB GPUs

### 实验结果

#### 单任务

![image.png](./1717555299005-76e6719b-0b85-4d6f-abdc-d9ed82353473.png)

#### 多任务

![image.png](./1717555310422-c8065eac-9980-4ded-801d-3adcef6eec56.png)

结论

- 由R-Tuning > Pretrain-W可知，R-Tuning在愿意回答部分的准确率优于各baseline
- 模型越大，改善越明显说明随着模型增大，知识gap更大

![image.png](./1717555400019-e5788791-9584-4727-8cd3-d8d0e0fa83ed.png)

结论

- 高AP值说明模型成功地对于自己给出的正确答案相较于幻觉由更高的confidence
- 进一步说明模型能够成功识别自己不能回答的问题
- 由于OOD的结果也更好了，可以得出拒绝回答是一种元技能（meta skill）

### Case study

![image.png](./1717555545562-e58e6c9d-d7d7-4844-862d-92a9c101cbe2.png)

## 方法变体：R-Tuning-U

除了用标签和回复进行匹配外，本文还提出一种无监督划分的变体：同一个问题推理多次，根据多次回答之间的一致性进行划分。

具体而言，推理k=10次，计算uncertainty *u：*

*![image.png](./1717555663145-2b2d378a-dcdb-4d43-a392-8e3f6873aa13.png)*

并将前50%划分为$D_0$，后50%为$D_1$

![image.png](./1717555722328-151723dc-c840-4177-b08d-225f9a595326.png)

新引入的几个baseline：

- Vanilla-C：推理k次 并进行 majority vote
- Vanilla-C:推理k次 & majority vote

结论

- R-Tuning-U > Vanilla-C 说明R-Tuning-U使得模型的回答准确率更高
- R-Tuning-U 结果的高AP值说明了用uncertainty 作为依据进行划分的可行性

## PPT

[组会PPT-2024.05.27-RTuning.pptx](https://yuque.zju.edu.cn/office/yuque/0/2024/pptx/30883/1717556108230-e14d9b57-4a2d-4cd7-a9c9-2de68c03ee2c.pptx?from=https%3A%2F%2Fyuque.zju.edu.cn%2Fzjuvag%2Fseminar%2Fli1cyi%2Fedit)