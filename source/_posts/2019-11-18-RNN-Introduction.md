---
title: å¾ªç¯ç¥ç»ç½‘ç»œ (Recurrent Neural Network, RNN)
tags: ["æŠ¥å‘Š", "ä¸»é¢˜æŠ¥å‘Š"]
date: 2019-11-18
author: å¼ å»ºä¼Ÿ
mail: zjw.cs@zju.edu.cn
mathjax: true
---

## 1. å¾ªç¯ç¥ç»ç½‘ç»œ (Recurrent Neural Network, RNN)

åœ¨æœºå™¨å­¦ä¹ ä¸­, æ•°æ®è¡¨ç¤ºä¸º $n$ ç»´ç‰¹å¾å‘é‡ $\mathbf{x}\in\mathbb{R}^n$ æˆ– $h\times w$ ç»´ç‰¹å¾çŸ©é˜µ(å¦‚å›¾ç‰‡), å¤šå±‚æ„ŸçŸ¥æœº (multilayer perceptron, MLP) å’Œå·ç§¯ç¥ç»ç½‘ç»œ (convolutional neural network, CNN) å¯ä»¥æå–æ•°æ®ä¸­çš„ç‰¹å¾ä»¥è¿›è¡Œåˆ†ç±»å›å½’ç­‰ä»»åŠ¡.
ä½†é€šå¸¸çš„ MLP æˆ– CNN å¤„ç†çš„æ•°æ®é€šå¸¸è®¤ä¸ºæ˜¯ç‹¬ç«‹åŒåˆ†å¸ƒçš„, å› æ­¤å½“æ•°æ®ä¹‹é—´å­˜åœ¨å…³è”å…³ç³»æ—¶, è¿™ç±»æ¨¡å‹åˆ™æ— æ³•å¾ˆå¥½çš„ç¼–ç æ•°æ®é—´çš„ä¾èµ–å…³ç³», å¯¼è‡´æ¨¡å‹çš„è¡¨ç°è¾ƒå·®. ä¸€ç§å…¸å‹çš„æ•°æ®é—´ä¾èµ–å…³ç³»å°±æ˜¯*æ—¶åºå…³ç³»*. æ¯”å¦‚è¯è¯´ä¸€åŠæ—¶å¯¹æ–¹å¯èƒ½å°±çŸ¥é“äº†ä½ çš„æ„æ€, ä¸€å¥è¯ä¸­çš„ä»£è¯"ä»–"æŒ‡ä»£çš„ç›®æ ‡éœ€è¦åˆ†æä¸Šä¸‹æ–‡åæ‰èƒ½å¾—åˆ°. æ—¶åºæ•°æ®å¦‚ä¸‹å›¾æ‰€ç¤º, ä¸€åˆ—å‘é‡åˆ™è¡¨ç¤ºä¸€ä¸ªå­—çš„ç¼–ç .

![æ—¶åºæ•°æ®](https://www.jarvis73.cn/images/2018-1-24/time-series.png)

å¾ªç¯ç¥ç»ç½‘ç»œçš„æå‡ºå°±æ˜¯ä¸ºäº†è§£å†³æ•°æ®ä¸­è¿™ç§å…¸å‹çš„æ—¶åºä¾èµ–å…³ç³». RNN æ˜¯å†…éƒ¨åŒ…å«å¾ªç¯çš„ç¥ç»ç½‘ç»œ (æ™®é€š CNN ä¸åŒ…å«å¾ªç¯), RNN çš„ä¸€ä¸ªå¾ªç¯å•å…ƒå¦‚ä¸‹å›¾æ‰€ç¤º[^1].

![RNN çš„å¾ªç¯å•å…ƒ](https://www.jarvis73.cn/images/2018-1-24/RNN-rolled.png)

å…¶ä¸­ $x_t$ è¡¨ç¤ºè¾“å…¥åºåˆ—ä¸­æ—¶åˆ» $t$ æ—¶çš„å€¼, $h_t$ ä¸ºè¯¥å±‚åœ¨æ—¶åˆ» $t$ çš„è¾“å‡º. æ–¹å— $A$ æ˜¯ä¸€ä¸ªæ“ä½œç¬¦, æŠŠå‰ä¸€æ—¶åˆ»çš„è¾“å‡º $h_{t-1}$ å’Œå½“å‰æ—¶åˆ»çš„è¾“å…¥ $x_t$ æ˜ å°„ä¸ºå½“å‰æ—¶åˆ»çš„è¾“å‡º. æ³¨æ„, $h_t$ é€šå¸¸æ‰®æ¼”ä¸¤ä¸ªè§’è‰², æ—¢æ˜¯å¾ªç¯å•å…ƒåœ¨å½“å‰æ—¶åˆ»çš„è¾“å‡º, åˆæ˜¯å½“å‰æ—¶åˆ»å¾ªç¯å•å…ƒçš„*çŠ¶æ€*. å…¬å¼è¡¨ç¤ºå¦‚ä¸‹:

$$
h_t = \sigma(W_{hx}x_t + W_{hh}h_{t-1}).
$$

å…¶ä¸­ $W$ ä¸ºæƒé‡å‚æ•°, $\sigma$ è¡¨ç¤ºæ¿€æ´»å‡½æ•°, å¸¸ç”¨çš„æ˜¯ $\tanh(\cdot)$ å‡½æ•°, å¯ä»¥æŠŠè¾“å‡ºçš„å€¼åŸŸæ§åˆ¶åœ¨ $[-1, 1]$ ä¹‹é—´, é¿å…åœ¨å¾ªç¯è¿‡ç¨‹ä¸­ä¸æ”¶æ•›. æˆ‘ä»¬å¯ä»¥æ²¿ç€æ—¶é—´è½´æŠŠä¸Šé¢çš„å¾ªç¯å•å…ƒå±•å¼€, æ›´åŠ ç›´è§‚.

![RNN å¾ªç¯å•å…ƒå±•å¼€ç¤ºæ„å›¾](https://www.jarvis73.cn/images/2018-1-24/RNN-unrolled.png)

å¾ªç¯ç¥ç»ç½‘ç»œå¯ä»¥ç”±å¤šå±‚å¾ªç¯å•å…ƒå †å è€Œæˆ, å‰ä¸€ä¸ªå¾ªç¯å•å…ƒçš„è¾“å‡ºä½œä¸ºä¸‹ä¸€å¾ªç¯å•å…ƒçš„è¾“å…¥, å¦‚ä¸‹å›¾æ‰€ç¤º.

<!-- åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­, ... -->

![å¤šå±‚å¾ªç¯å•å…ƒå †å ](https://www.jarvis73.cn/images/2018-1-24/multilayer-RNN.png)

RNN çš„è¾“å…¥é€šå¸¸è¡¨ç¤ºæˆ**åµŒå…¥ (embedding)**çš„å½¢å¼, å³æ„é€ ä¸€ä¸ª**æŸ¥è¯¢è¡¨ (lookup table)**, æŠŠè¾“å…¥åºåˆ—çš„æ¯ä¸ªæ—¶åˆ»çš„ç‰¹å¾å‘é‡é€šè¿‡æŸ¥è¯¢è¡¨è½¬ä¸ºä¸€ä¸ªç­‰é•¿çš„å‘é‡. ä»è€Œä¸€ä¸ªåºåˆ—çš„å½¢çŠ¶å˜ä¸º `[num_time_steps, embedding_size]`.

### 1.1 RNN çš„åº”ç”¨

RNN å¯ä»¥æ ¹æ®è¾“å…¥åºåˆ—çš„é•¿åº¦å’Œè¾“å‡ºåºåˆ—çš„é•¿åº¦åˆ†ä¸ºä¸‰å¤§ç±».

-   å¤šå¯¹ä¸€: å¸¸ç”¨äºæƒ…æ„Ÿåˆ†æ, æ–‡æœ¬åˆ†ç±»
-   ä¸€å¯¹å¤š: Image Caption
-   å¤šå¯¹å¤š: æœºå™¨ç¿»è¯‘
-   ä¸€å¯¹ä¸€: é€€åŒ–ä¸º MLP

### 1.2 RNN çš„å±€é™æ€§

RNN ä¹Ÿå­˜åœ¨ä¸€äº›ç¼ºé™·:

-   RNN å¯ä»¥å¾ˆå¥½çš„å­¦ä¹ åºåˆ—ä¸­é‚»è¿‘æ—¶é—´æ­¥æ•°æ®ç‚¹(çŸ­æœŸ)ä¹‹é—´çš„å…³ç³», ä½†å¯¹äºé•¿æœŸä¾èµ–ä¼šå˜å¾—ä¸ç¨³å®š.
-   RNN å¯ä»¥æŠŠå›ºå®šé•¿åº¦çš„è¾“å…¥åºåˆ—æ˜ å°„åˆ°æŒ‡å®šé•¿åº¦çš„è¾“å‡ºåºåˆ—, ä½†ä¸èƒ½åŠ¨æ€åœ°æ ¹æ®è¾“å…¥å†³å®šè¾“å‡ºå¤šé•¿çš„åºåˆ—.

è€Œ LSTM å’Œ Encoder-Decoder çš„æå‡ºè§£å†³äº†è¿™ä¸¤ä¸ªé—®é¢˜.

## 2. é•¿çŸ­æœŸè®°å¿† (Long Short Term Memory, LSTM)[^1]

å‰é¢æåˆ°, RNN å¯¹äºé•¿æœŸä¾èµ–ç»å®éªŒè¡¨æ˜æ˜¯ä¸ç¨³å®šçš„. å¯¹äºçŸ­åºåˆ—, å¦‚ä¸€ä¸ªå¥å­: "The clouds are in the ()", æ‹¬å·ä¸­é¢„æµ‹ä¸€ä¸ªè¯, é‚£ä¹ˆå¾ˆå®¹æ˜“æ ¹æ®è¯¥è¯å‰é¢çš„ clouds å’Œ in æ¨æ–­å‡ºå¡« sky. ä½†æ˜¯å¯¹äºé•¿åºåˆ—, å¦‚ "I grew up in France ... I speak fluent ()", å¥å­ä¸­çš„çœç•¥å·åŒ…å«äº†å¤§é‡å…¶ä»–ä¿¡æ¯, æ­¤æ—¶æœ€åæ‹¬å·ä¸­çš„è¯åº”å½“æ ¹æ®å¼€å¤´çš„ France æ¨æ–­ä¸º French, ä½†ä¸­é—´å¤§é‡çš„æ— ç”¨è¯­å¥ä¼šç¨€é‡Šå‰æœŸçš„ä¿¡æ¯, å¯¼è‡´ RNN æ— æ³•æ­£ç¡®é¢„æµ‹æœ€åçš„è¯. è€Œ Hochreiter & Schmidhuber æå‡ºçš„ LSTM [^6] æ­£æ˜¯è§£å†³è¯¥é—®é¢˜çš„.

LSTM å’Œé€šå¸¸çš„ CNN ä¸€æ ·ä¸ºä¸€ä¸ªå¾ªç¯å•å…ƒçš„ç»“æ„, ä½†æ˜¯ä¸ RNN ä»…æœ‰ä¸€ä¸ª tanh æ¿€æ´»å±‚ä¸åŒ, LSTM ä¸­åŒ…å«äº†æ›´å¤æ‚çš„å››å±‚ç½‘ç»œçš„ç»“æ„è®¾è®¡, å¹¶ä¸”å››å±‚ç½‘ç»œç›¸äº’è€¦åˆ, å¦‚ä¸‹å›¾æ‰€ç¤º.

![LSTM å¾ªç¯å•å…ƒå±•å¼€ç¤ºæ„å›¾](https://www.jarvis73.cn/images/2018-1-24/LSTM.png)

ä¸Šå›¾ä¸­çš„åœ†è§’çŸ©å½¢æ¡†, æ“ä½œç¬¦, åˆ†æ”¯ç®­å¤´çš„å«ä¹‰å¦‚ä¸‹å›¾æ‰€ç¤º.

![å›¾ä¾‹](https://www.jarvis73.cn/images/2018-1-24/LSTM2-notation.png)

ä¸‹é¢è¯¦ç»†ä»‹ç» LSTM å•å…ƒçš„å†…éƒ¨ç»“æ„.

### 2.1 LSTM çš„æ ¸å¿ƒæ€æƒ³

LSTM ç›¸æ¯”äº RNN, å…³é”®åœ¨äºå¼•å…¥äº†å•å…ƒçŠ¶æ€(state) $C$ â€”â€” æ¨ªç©¿ä¸‹å›¾é¡¶éƒ¨çš„ç›´çº¿.

![å•å…ƒçŠ¶æ€](https://www.jarvis73.cn/images/2018-1-24/LSTM3-C-line.png)

LSTM å¯ä»¥é€šè¿‡**é—¨(gate)**æ¥æ§åˆ¶å‘å•å…ƒçŠ¶æ€ä¸­å¢åŠ ä¿¡æ¯æˆ–å‡å°‘ä¿¡æ¯. é—¨ç”±ä¸€ä¸ª $sigmoid$ å‡½æ•°å’Œä¸€ä¸ªä¹˜æ³•è¿ç®—ç¬¦ç»„æˆ, å¦‚ä¸‹å›¾æ‰€ç¤º.

![é—¨](https://www.jarvis73.cn/images/2018-1-24/LSTM3-gate.png)

$sigmoid$ å±‚è¾“å‡ºçš„å€¼åœ¨ $[0, 1]$ ä¹‹é—´, æ§åˆ¶äº†ä¿¡æ¯çš„é€šè¿‡é‡. è¶Šæ¥è¿‘ 0, åˆ™è¡¨æ˜ä¸å…è®¸ä¿¡æ¯é€šè¿‡(ä»è€Œå½¢æˆ*é—å¿˜*); è¶Šæ¥è¿‘ 1, åˆ™è¡¨æ˜å…è®¸ä¿¡æ¯å…¨éƒ¨é€šè¿‡(ä»è€Œå½¢æˆ*è®°å¿†*).

### 2.2 LSTM å•å…ƒè§£æ

LSTM å•å…ƒåœ¨æ¯ä¸ªæ—¶é—´æ­¥éœ€è¦æ³¨æ„ä¸‰ä¸ªå‘é‡:

-   è¾“å…¥çš„ç‰¹å¾å‘é‡ $x_t$
-   ä¸Šä¸€æ­¥è¾“å‡ºçš„ç‰¹å¾å‘é‡ $h_{t-1}$
-   ä¸Šä¸€æ­¥ç»“æŸåçš„å•å…ƒçŠ¶æ€ $C_{t-1}$

è¦æ³¨æ„ä¸‰ä¸ªå‘é‡æ˜¯ç›¸åŒçš„é•¿åº¦.

**é—å¿˜é—¨(forget gate).** æ¯å¾ªç¯ä¸€æ­¥æ—¶, é¦–å…ˆæ ¹æ®ä¸Šä¸€æ­¥çš„è¾“å‡º $h_{t-1}$ å’Œå½“å‰æ­¥çš„è¾“å…¥ $x_t$ æ¥å†³å®šè¦é—å¿˜æ‰ä¸Šä¸€æ­¥çš„ä»€ä¹ˆä¿¡æ¯(ä»å•å…ƒçŠ¶æ€ $C_{t-1}$ ä¸­é—å¿˜). å› æ­¤åªéœ€è¦è®¡ç®—ä¸€ä¸ªé—å¿˜ç³»æ•° $f_t$ ä¹˜åˆ°å•å…ƒçŠ¶æ€ä¸Šå³å¯. å¦‚ä¸‹å›¾æ‰€ç¤º, å…¬å¼ä¸­çš„æ–¹æ‹¬å·è¡¨ç¤º $concat$ æ“ä½œ.

![é—å¿˜é—¨](https://www.jarvis73.cn/images/2018-1-24/LSTM3-focus-f.png)

**è¾“å…¥é—¨(input gate).** è¿™ä¸€æ­¥æ¥å†³å®šå½“å‰æ–°çš„è¾“å…¥ $x_t$ æˆ‘ä»¬åº”è¯¥æŠŠå¤šå°‘ä¿¡æ¯å‚¨å­˜åœ¨å•å…ƒçŠ¶æ€ä¸­. è¿™éƒ¨åˆ†æœ‰ä¸¤æ­¥, é¦–å…ˆä¸€ä¸ªè¾“å…¥é—¨è®¡ç®—è¦ä¿ç•™å“ªäº›ä¿¡æ¯, å¾—åˆ°è¿‡æ»¤ç³»æ•° $i_t$, ç„¶åä½¿ç”¨ä¸€ä¸ªå…¨è¿æ¥å±‚æ¥ä»ä¸Šä¸€æ­¥çš„è¾“å‡º $h_{t-1}$ å’Œå½“å‰æ­¥çš„è¾“å…¥ $x_t$ ä¸­æå–ç‰¹å¾ $\tilde{C}_t$. å¦‚ä¸‹å›¾æ‰€ç¤º.

![è¾“å…¥é—¨](https://www.jarvis73.cn/images/2018-1-24/LSTM3-focus-i.png)

**æ–°æ—§ä¿¡æ¯åˆå¹¶.** è®¡ç®—å¥½äº†é—å¿˜ç³»æ•°, è¾“å…¥ç³»æ•°, ä»¥åŠæ–°çš„è¦è®°å¿†çš„ç‰¹å¾, ç°åœ¨å°±å¯ä»¥åœ¨å•å…ƒçŠ¶æ€ $C_{t-1}$ ä¸Šæ‰§è¡Œé—å¿˜æ“ä½œ $f_t\ast C_{t-1}$ å’Œè®°å¿†æ“ä½œ $+i_t\ast\tilde{C}_t$. å¦‚ä¸‹å›¾æ‰€ç¤º.

![æ–°æ—§ä¿¡æ¯åˆå¹¶](https://www.jarvis73.cn/images/2018-1-24/LSTM3-focus-C.png)

**è¾“å‡ºé—¨(output gate).** æœ€åæˆ‘ä»¬è¦å†³å®šè¾“å‡ºä»€ä¹ˆä¿¡æ¯äº†. è¿™éœ€è¦ä»å½“å‰çš„å•å…ƒçŠ¶æ€ $C_t$ æ¥è·å–è¦è¾“å‡ºçš„ä¿¡æ¯. ä½†æ˜¾ç„¶æˆ‘ä»¬å¹¶ä¸ä¼šåœ¨è¿™ä¸€ä¸ªæ—¶é—´æ­¥è¾“å‡ºæ‰€æœ‰è®°å¿†çš„ä¿¡æ¯, è€Œæ˜¯åªè¦è¾“å‡ºå½“å‰éœ€è¦çš„ä¿¡æ¯, å› æ­¤æˆ‘ä»¬ç”¨ä¸€ä¸ªè¾“å‡ºé—¨æ¥è¿‡æ»¤è¾“å‡ºçš„ä¿¡æ¯, è¿‡æ»¤ç³»æ•°ä¸º $o_t$. æ­¤å¤–æˆ‘ä»¬å¸Œæœ›è¾“å‡ºçš„ç‰¹å¾çš„å–å€¼èƒ½å¤Ÿä»‹äº $[-1, 1]$ ä¹‹é—´, å› æ­¤ä½¿ç”¨ä¸€ä¸ª $tanh$ å‡½æ•°æŠŠå•å…ƒçŠ¶æ€ $C_t$ æ˜ å°„åˆ°ç›¸åº”çš„èŒƒå›´, æœ€åä¹˜ä¸Šè¿‡æ»¤ç³»æ•°å¾—åˆ°å½“å‰æ­¥çš„è¾“å‡º. å¦‚ä¸‹å›¾æ‰€ç¤º.

![è¾“å‡ºé—¨](https://www.jarvis73.cn/images/2018-1-24/LSTM3-focus-o.png)

### 2.3 LSTM å•å…ƒå˜ä½“

**å˜ä½“ä¸€.** [^7]è®©æ‰€æœ‰çš„é—¨æ§å•å…ƒåœ¨è¾“å‡ºé—¨æ§ç³»æ•°çš„æ—¶å€™éƒ½å¯ä»¥"çœ‹åˆ°"å½“å‰çš„å•å…ƒçŠ¶æ€. å¦‚ä¸‹å›¾æ‰€ç¤º.

![è®©é—¨æ§å•å…ƒå¯ä»¥çœ‹åˆ°å•å…ƒçŠ¶æ€](https://www.jarvis73.cn/images/2018-1-24/LSTM3-var-peepholes.png)

**å˜ä½“äºŒ.** è®©é—å¿˜é—¨çš„é—å¿˜ç³»æ•° $f_t$ å’Œè¾“å…¥é—¨çš„è¾“å…¥ç³»æ•° $i_t$ è€¦åˆ, å³ä»¤ $i_t = 1-f_t$, ä»è€ŒåŒæ—¶åšå‡ºå“ªäº›ä¿¡æ¯é—å¿˜ä»¥åŠå“ªäº›ä¿¡æ¯è®°å¿†çš„å†³ç­–. è¿™ä¸ªå˜ä½“å¯ä»¥è®©æ–°çš„æœ‰ç”¨çš„è®°å¿†"è¦†ç›–"è€çš„æ— ç”¨çš„è®°å¿†. å¦‚ä¸‹å›¾æ‰€ç¤º.

![é—å¿˜ç³»æ•°å’Œè®°å¿†ç³»æ•°è€¦åˆ](https://www.jarvis73.cn/images/2018-1-24/LSTM3-var-tied.png)

**å˜ä½“ä¸‰(GRU).** [^8]ç¬¬ä¸‰ç§å˜ä½“æ›´ä¸ºæœ‰å, ç§°ä¸º**é—¨æ§å¾ªç¯å•å…ƒ(Gated Recurrent Unit, GRU)**. ä¸‹ä¸€èŠ‚ä»‹ç».

**å…¶ä»–å‚è€ƒ:** å…¶ä»–å¯ä»¥å‚è€ƒå¦‚ä¸‹æ–‡çŒ®:

-   [Depth Gated RNNs](http://arxiv.org/pdf/1508.03790v2.pdf)
-   [Variants comparision](http://arxiv.org/pdf/1503.04069.pdf)
-   [Ten thousand RNN architecture tests](http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)

## 3. é—¨æ§å¾ªç¯å•å…ƒ(Gated Recurrent Unit, GRU)

GRU[^8] æ˜¯ä¸€ç§æ¯” LSTM ç¨ç®€å•ä¸€äº›çš„å¾ªç¯å•å…ƒ. GRU æŠŠ LSTM ä¸­çš„éšè—çŠ¶æ€ $h$ å’Œå•å…ƒçŠ¶æ€ $C$ åˆå¹¶ä¸ºå•ä¸ªçš„éšè—çŠ¶æ€. å¦‚ä¸‹å›¾æ‰€ç¤º.

![é—¨æ§å¾ªç¯å•å…ƒ (GRU)](https://www.jarvis73.cn/images/2018-1-24/LSTM3-var-GRU.png)

**æ›´æ–°é—¨(update gate).** æ›´æ–°é—¨ç³»æ•° $z_t$ æ§åˆ¶äº† $h_{t-1}$ ä¸­ä¿å­˜çš„ä¿¡æ¯(å¦‚é•¿æœŸè®°å¿†)åœ¨å½“å‰æ­¥ä¿ç•™å¤šå°‘. $z_t$ æ¥è¿‘ 0 æ—¶ä¸Šä¸€æ­¥çš„éšè—çŠ¶æ€ä¸­çš„ä¿¡æ¯ $h_{t-1}$ å¾—ä»¥ä¿ç•™, æ–°è¾“å…¥çš„ä¿¡æ¯ $\tilde{h}_t$ ä¼šè¢«å¿½ç•¥; $z_t$ æ¥è¿‘ 1 æ—¶åˆ™ä¸¢å¼ƒå·²æœ‰çš„ä¿¡æ¯, å¹¶å¡«å…¥æ–°è¾“å…¥çš„ä¿¡æ¯.

**è¾“å…¥ä¿¡æ¯çš„åŠ å·¥.** å½“å‰æ­¥è¾“å…¥çš„ä¿¡æ¯éœ€è¦åŠ å·¥åæ‰èƒ½åˆå¹¶åˆ°éšè—çŠ¶æ€ $h_t$ ä¸­. è¾“å…¥ä¿¡æ¯åŠ å·¥æ—¶éœ€è¦å‚è€ƒä¸Šä¸€æ­¥çš„éšè—çŠ¶æ€ $h_{t-1}$ æ¥å†³å®šå“ªäº›ä¿¡æ¯æœ‰ç”¨, å“ªäº›æ²¡ç”¨. åŠ å·¥åçš„ä¿¡æ¯ç”¨ $\tilde{h}_t$ è¡¨ç¤º.

**é‡ç½®é—¨(reset gate).** é‡ç½®é—¨ç³»æ•° $r_t$ æ§åˆ¶äº†åœ¨åŠ å·¥è¾“å…¥ä¿¡æ¯çš„æ—¶å€™ä½¿ç”¨ä¸Šä¸€æ­¥çš„éšè—çŠ¶æ€ä¸­çš„å“ªäº›ä¿¡æ¯. $r_t$ æ¥è¿‘ 0 æ—¶æ–°è¾“å…¥çš„ä¿¡æ¯å ä¸»å¯¼åœ°ä½, è¯´æ˜å½“å‰æ­¥çš„è¾“å…¥åŒ…å«çš„ä¿¡æ¯ä¸å‰é¢çš„ä¿¡æ¯å…³è”æ€§å¾ˆå°; $r_t$ æ¥è¿‘ 1 æ—¶æ–°è¾“å…¥çš„ä¿¡æ¯å’Œå‰é¢çš„é•¿æœŸä¿¡æ¯æœ‰è¾ƒå¤§å…³è”æ€§, éœ€è¦ç»¼åˆè€ƒè™‘æ¥äº§ç”Ÿå½“å‰æ­¥åŠ å·¥åçš„ä¿¡æ¯.

## 4. ç¼–ç -è§£ç å™¨ (Encoder-Decoder)

ç¼–ç -è§£ç å™¨æ¨¡å‹æ˜¯ä¸ºäº†å®ç° $n\rightarrow m$ åºåˆ—æ˜ å°„çš„æ¨¡å‹æ¡†æ¶. è¿™ç±»æ¨¡å‹ä¹Ÿç§°ä¸º Sequence to Sequence(seq2seq). ç¼–ç å™¨åªè´Ÿè´£å¤„ç†è¾“å…¥åºåˆ—, å¹¶å½¢æˆè¾“å…¥åºåˆ—çš„ç‰¹å¾å‘é‡åé€å…¥è§£ç å™¨. ä¸ºäº†æ¸…æ¥š, æˆ‘ä»¬ç”¨å…¬å¼æ¥è¡¨ç¤ºè¿™ä¸ªè¿‡ç¨‹[^5]. å¯¹äºè¾“å…¥åºåˆ— $X=\\{x_1, x_2, \cdots, x_S\\}$ å’ŒæœŸæœ›çš„è¾“å‡ºåºåˆ— $Y=\\{y_1, y_2, \cdots, y_T\\}$, æˆ‘ä»¬ä½¿ç”¨ RNN æ¨¡å‹å¯¹å…¶è¿›è¡Œå»ºæ¨¡, å½¢æˆä¸€ä¸ªæ¡ä»¶æ¦‚ç‡ $P(Y\vert X)$, ä½¿ç”¨é“¾å¼æ³•åˆ™è§£è€¦å¦‚ä¸‹:

$$
P(Y|X) = \prod_{t=1}^TP(y_t\lvert y_1, y_2, \cdots, y_{t-1}, X).
$$

é‚£ä¹ˆè¯¥ RNN æ¨¡å‹å°±æ˜¯ä¸€ä¸ªç¼–ç å™¨, åœ¨æ—¶åˆ» $s$ çš„çŠ¶æ€é€šè¿‡ä¸‹å¼è®¡ç®—:

$$
h_s = f_{enc}(h_{s-1}, x_s).
$$

ç¼–ç å™¨çš„ç¤ºæ„å›¾å¦‚ä¸‹å›¾[^2]æ‰€ç¤º:

![ç¼–ç å™¨. è¾“å‡ºçš„è¯­ä¹‰å‘é‡å¯ä»¥é€šè¿‡ä¸åŒçš„è®¡ç®—å…¬å¼æ„é€ , å½¢æˆä¸åŒçš„æ¨¡å‹ç»“æ„](https://www.jarvis73.cn/images/2018-1-24/encoder.png)

è§£ç å™¨ RNN æ¯ä¸ªæ—¶é—´æ­¥ä½¿ç”¨å‰ä¸€æ­¥çš„è¾“å‡º $y_{t-1}$ å’Œå½“å‰çŠ¶æ€ $g_t$ äº§ç”Ÿä¸€ä¸ªè¾“å‡º $y_t\in Y$:

$$
\begin{align}
g_1 &= h_s \\
g_t &= f_{dec}(g_{t-1}, y_{t-1})
\end{align}
$$

æ¯ä¸ªæ—¶é—´æ­¥çš„è¾“å‡ºæ¦‚ç‡é€šè¿‡ä¸€ä¸ªçº¿æ€§å±‚å’Œä¸€ä¸ª softmax å‡½æ•°å¾—åˆ°:

$$
P(y_t\lvert y_1, y_2, \cdots, y_{t-1}, X) = Softmax(Linear(g_t)).
$$

ä½¿ç”¨è§£ç å™¨å¯¹è¯­ä¹‰å‘é‡è§£ç . å¦‚æœæŠŠè¯­ä¹‰å‘é‡åªè¾“å…¥è§£ç å™¨çš„ç¬¬ä¸€ä¸ªå¾ªç¯æ—¶é—´æ­¥, åˆ™å½¢æˆäº† Cho et al.[^3] æå‡ºçš„ç»“æ„.

![è§£ç å™¨-1](https://www.jarvis73.cn/images/2018-1-24/decoder-1.png)

å¦‚æœæŠŠè¯­ä¹‰å‘é‡åœ¨æ¯ä¸€ä¸ªæ—¶é—´æ­¥éƒ½è¾“å…¥è§£ç å™¨, åˆ™å½¢æˆäº† Sutskever et al.[^4] æå‡ºçš„ç»“æ„.

![è§£ç å™¨-2](https://www.jarvis73.cn/images/2018-1-24/decoder-2.png)

### 4.1 ç¼–ç -è§£ç å™¨æ¨¡å‹çš„å±€é™æ€§

-   ä¿¡æ¯çš„ä¸¢å¤±: æ•´ä¸ªæ—¶é—´åºåˆ—åªèƒ½å‹ç¼©ä¸ºä¸€ä¸ªå›ºå®šé•¿åº¦çš„è¯­ä¹‰å‘é‡
-   ä¸åˆç†æ€§: seq2seq çš„ä»»åŠ¡ä¸­è¾“å…¥åºåˆ— $\\{x_0, x_1, \dots, x_{tâˆ’1},  x_ğ‘¡, x_{t+1},\dots \\}$ ä¸­çš„æ¯ä¸ªå…ƒç´ å¯¹æ‰€æœ‰ $y_s$ çš„è´¡çŒ®åº¦æ˜¯ç›¸åŒçš„

ä¾‹å¦‚: The animal didn't cross the street because **it** was too tired. åœ¨è¿™å¥è¯ä¸­, äººæ˜¯é€šè¿‡ç»¼åˆæ•´å¥è¯çš„ä¿¡æ¯æ¥åˆ¤æ–­å•è¯ it æŒ‡ä»£çš„æ˜¯ the animal, ä»è€Œç¿»è¯‘æ—¶ the animal åº”è¯¥å¯¹ it çš„å½±å“æ›´å¤§.

äººä»¬æå‡ºäº†æ³¨æ„åŠ›æ¨¡å‹æ¥è§£å†³æ™®é€šç¼–ç -è§£ç å™¨æ¨¡å‹çš„é—®é¢˜.

## 5. æ³¨æ„åŠ›æœºåˆ¶ (Attention Mechanism)[^11]

### 5.1 ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›?

å¿ƒç†å­¦ä¸­å¯¹æ³¨æ„åŠ›çš„è§£é‡Šæ˜¯:

> **Attention**Â is the behavioral andÂ cognitive processÂ of selectively concentrating on a discrete aspect of information, whether deemed subjective or objective, while ignoring other perceivable information.

æ³¨æ„åŠ›æ˜¯æŠŠæœ‰é™çš„èµ„æºé›†ä¸­åœ¨æ›´é‡è¦çš„ç›®æ ‡ä¸Š. æ³¨æ„åŠ›æœºåˆ¶çš„ä¸¤ä¸ªè¦ç´ :

-   å†³å®šè¾“å…¥ä¿¡æ¯çš„å“ªéƒ¨åˆ†æ˜¯é‡è¦çš„
-   æŠŠèµ„æºé›†ä¸­åˆ†é…åˆ°é‡è¦çš„ä¿¡æ¯ä¸Š

æ²¿ç”¨ç¼–ç -è§£ç å™¨æ¨¡å‹çš„ç»“æ„, æ³¨æ„åŠ›æœºåˆ¶é€šè¿‡å¼•å…¥ä¸€ç»„å½’ä¸€åŒ–çš„ç³»æ•° $\\{\alpha_1, \alpha_2, \dots, \alpha_n \\}$ æ¥å¯¹è¾“å…¥çš„ä¿¡æ¯è¿›è¡Œé€‰æ‹©, æ¥è§£å†³ç¼–ç -è§£ç å™¨çš„ä¸åˆç†æ€§. è¿™é‡Œè¾“å…¥çš„ä¿¡æ¯å°±æ˜¯æŒ‡è¾“å…¥åºåˆ—åœ¨ RNN ä¸­çš„å•å…ƒè¾“å‡º $\mathbf{h}_s$. å½’ä¸€åŒ–çš„ç³»æ•° $\alpha_s$ ç”¨æ¥å†³å®šè¾“å…¥ä¿¡æ¯çš„é‡è¦æ€§, æ˜¯ç¼–ç å™¨è¾“å‡ºæ—¶å¯¹å•å…ƒè¾“å‡ºåŠ æƒæ±‚å’Œç³»æ•°. æ³¨æ„åŠ›æœºåˆ¶åœ¨è®¡ç®—ä¸åŒæ—¶é—´æ­¥çš„è¾“å‡ºæ—¶, å®æ—¶æ„é€ ç¼–ç å™¨è¾“å‡ºçš„è¯­ä¹‰å‘é‡ $\mathbf{c}_t$, ä»è€Œè§£å†³äº†æ™®é€šç¼–ç -è§£ç å™¨ä¿¡æ¯ä¸¢å¤±çš„é—®é¢˜. æ³¨æ„åŠ›æœºåˆ¶å¦‚ä¸‹å›¾æ‰€ç¤º.

![æ³¨æ„åŠ›æœºåˆ¶](https://www.jarvis73.cn/images/2018-1-24/attention-1.png)

ä¸‹é¢è®¨è®ºåŠ æƒç³»æ•°æ˜¯å¦‚ä½•è®¡ç®—çš„. è€ƒè™‘åˆ°åŠ æƒç³»æ•°è¦åæ˜ **è¾“å…¥ä¿¡æ¯**åœ¨å½“å‰**æ—¶é—´æ­¥**ä¸Šçš„é‡è¦æ€§, å› æ­¤éœ€è¦æŠŠè¾“å…¥ä¿¡æ¯å’Œæ—¶é—´ä¿¡æ¯ç»“åˆèµ·æ¥è®¡ç®—åŠ æƒç³»æ•°. å› æ­¤é€šå¸¸ä½¿ç”¨ä¸€ä¸ª MLP æ¥æ”¶è¾“å…¥ä¿¡æ¯ $\mathbf{h}\_1, \mathbf{h}\_2, \dots, \mathbf{h}\_n$ å’Œè§£ç å™¨ä¸Šä¸€æ—¶åˆ»çš„çŠ¶æ€ $\mathbf{s}\_{t-1}$ ä½œä¸ºè¾“å…¥, è¾“å‡ºå½’ä¸€åŒ–çš„åŠ æƒç³»æ•° $\alpha_{t1}, \alpha_{t2}, \dots, \alpha_{tn}$. å¦‚ä¸‹å›¾æ‰€ç¤º.

![æ³¨æ„åŠ›æœºåˆ¶](https://www.jarvis73.cn/images/2018-1-24/attention-2.png)

æ³¨æ„: ç¼–ç -è§£ç å™¨çš„ç»“æ„åªæ˜¯æ³¨æ„åŠ›æœºåˆ¶çš„ä¸€ä¸ªè½½ä½“, æ³¨æ„åŠ›æœºåˆ¶çš„æ ¸å¿ƒåœ¨äºåŠ æƒç³»æ•°, å› æ­¤å¯ä»¥ç”¨äºé RNN çš„ç»“æ„.

### 5.2 è‡ªæ³¨æ„åŠ› (Self-Attention)

è‡ªæ³¨æ„åŠ›æ˜¯ä¸€ä¸ªç¥ç»ç½‘ç»œæ¨¡å—, å®ƒä»ç„¶æ˜¯ä½¿ç”¨äº†æ³¨æ„åŠ›æœºåˆ¶, ä¸ç¼–ç -è§£ç å™¨ç»“æ„ä¸åŒçš„æ˜¯, è‡ªæ³¨æ„åŠ›æ¨¡å—åªä½¿ç”¨è¾“å…¥çš„ä¿¡æ¯è®¡ç®—åŠ æƒç³»æ•°, è€Œä¸éœ€è¦ä¸Šä¸€ä¸ªæ—¶é—´æ­¥çš„ä¿¡æ¯, å› æ­¤å¯ä»¥å®ç°æ›´å¤§è§„æ¨¡çš„å¹¶è¡Œè®¡ç®—. Google åœ¨ 2017 å¹´æå‡ºçš„ Transformer å®ç°äº†å¯å¹¶è¡Œçš„è‡ªæ³¨æ„åŠ›æ¨¡å—[^9]. å…¶ç»“æ„å¦‚ä¸‹å›¾æ‰€ç¤º.

![è¯­è¨€ç¿»è¯‘ä»»åŠ¡ä¸­çš„è‡ªæ³¨æ„åŠ›](https://www.jarvis73.cn/images/2018-1-24/self-attention.png)

è‡ªæ³¨æ„åŠ›æ¨¡å—å·¥ä½œæ–¹å¼å¦‚ä¸‹[^10]:

-   è¾“å…¥çš„åºåˆ—é¦–å…ˆè½¬åŒ–ä¸ºç›¸åŒé•¿åº¦çš„ embedding
-   åˆ›å»ºä¸‰ä¸ªçŸ©é˜µ: æŸ¥è¯¢çŸ©é˜µ (Query), é”®çŸ©é˜µ (Key), å€¼çŸ©é˜µ (Value)
-   ä½¿ç”¨è¿™ä¸‰ä¸ªçŸ©é˜µæŠŠæ¯ä¸ªå•è¯çš„ embedding æ˜ å°„ä¸ºä¸‰ä¸ªç¨çŸ­çš„ç‰¹å¾å‘é‡, åˆ†åˆ«ä»£è¡¨äº†å½“å‰å•è¯çš„æŸ¥è¯¢å‘é‡, é”®å‘é‡å’Œå€¼å‘é‡.
-   æ¯”å¦‚æˆ‘ä»¬è¦è®¡ç®—å•è¯ Thinking å…³äºå¥å­ä¸­å…¶ä»–å•è¯çš„æ³¨æ„åŠ›æ—¶, ä½¿ç”¨ Thinking çš„æŸ¥è¯¢å‘é‡ä¾æ¬¡ä¸å¥å­ä¸­æ‰€æœ‰å•è¯ (åŒ…æ‹¬ Thinking) çš„é”®å‘é‡åšç‚¹ç§¯æ¥è®¡ç®—ç›¸ä¼¼åº¦, å¹¶å¯¹ç›¸ä¼¼åº¦è¿›è¡Œå½’ä¸€åŒ–å¾—åˆ°åŠ æƒç³»æ•° (è¿™æ˜¯ attention çš„æ ¸å¿ƒéƒ¨åˆ†).
-   åŠ æƒç³»æ•°ä¹˜åˆ°æ‰€æœ‰å•è¯çš„å€¼å‘é‡ä¸Šæ¥å¾—åˆ°å•è¯ Thinking ç»è¿‡è‡ªæ³¨æ„åŠ›æ¨¡å—åè¾“å‡ºçš„ç‰¹å¾å‘é‡, è¿™ä¸ªç‰¹å¾å‘é‡ä¸­å¯ä»¥çœ‹ä½œåŒ…å«äº†ç¿»è¯‘ä»»åŠ¡é‡å¯¹å‡†ç¡®ç¿»è¯‘ Thinking æ‰€éœ€è¦çš„ä¿¡æ¯, è€Œä¸åŒ…å«å…¶ä»–ä¿¡æ¯ (å…¶ä»–ä¿¡æ¯åœ¨ç¿»è¯‘å…¶ä»–è¯æ—¶å¯èƒ½æœ‰ç”¨, ä½†åœ¨ç¿»è¯‘ Thinking æ—¶æ— ç”¨).

## References

[^1]:
    **Understanding LSTM Networks -- Colah's blogs** <br />
    [[link]](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) OnLine.

[^2]:
    **è¯¦è§£ä» Seq2Seq æ¨¡å‹ã€RNN ç»“æ„ã€Encoder-Decoder æ¨¡å‹ åˆ° Attention æ¨¡å‹** <br />
    [[link]](https://caicai.science/2018/10/06/attention%E6%80%BB%E8%A7%88/) OnLine.

[^3]:
    **Learning phrase representations using RNN encoder-decoder for statistical machine translation** <br />
    Cho K, Van MerriÃ«nboer B, Gulcehre C, et al. <br />
    [[link]](https://arxiv.org/abs/1406.1078) In arXiv preprint arXiv:1406.1078, 2014.

[^4]:
    **Sequence to Sequence Learning with Neural Networks** <br />
    Ilya Sutskever, Oriol Vinyals, Quoc V.Le. <br />
    [[link]](http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks) In Advances in neural information processing systems. 2014: 3104-3112.

[^5]:
    **Order Matters: Sequence to Sequence for Sets** <br />
    Vinyals, Oriol, Samy Bengio, and Manjunath Kudlur. <br />
    [[link]](http://arxiv.org/abs/1511.06391.) In ArXiv:1511.06391, November. 2015.

[^6]:
    **Long short-term memory** <br />
    Hochreiter S, Schmidhuber J. <br />
    [[link]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.676.4320&rep=rep1&type=pdf) In Neural computation, 1997, 9(8): 1735-1780.

[^7]:
    **Recurrent nets that time and count** <br />
    Gers F A, Schmidhuber J. <br />
    [[link]](ftp://ftp.idsia.ch/pub/juergen/TimeCount-IJCNN2000.pdf) In Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. 2000.

[^8]:
    **Learning phrase representations using RNN encoder-decoder for statistical machine translation** <br />
    Cho K, Van MerriÃ«nboer B, Gulcehre C, et al. <br />
    [[link]](https://arxiv.org/pdf/1406.1078) In arXiv preprint arXiv:1406.1078, 2014.

[^9]:
    **Attention is all you need** <br />
    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin <br />
    [[link]](https://arxiv.org/abs/1706.03762) Advances in neural information processing systems. 2017: 5998-6008.

[^10]:
    **The Illustrated Transformer** <br />
    Jay Alammar <br />
    [[link]](https://jalammar.github.io/illustrated-transformer/) OnLine.

[^11]:
    **Neural machine translation by jointly learning to align and translate** <br />
    Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio <br />
    [[link]](https://arxiv.org/abs/1409.0473) arXiv preprint arXiv:1409.0473, 2014.
