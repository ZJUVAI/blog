<!DOCTYPE html>
<html lang="en">
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="keyword"  content="">
    <link rel="shortcut icon" href="/blog/img/favicon.ico">

    <title>
        
        Introduction of Reinforcement learning - undefined
        
    </title>

    <!-- Custom CSS -->
    
<link rel="stylesheet" href="/blog/css/aircloud.css">
<link rel="stylesheet" href="/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/.css" src="/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/.js">

    
<link rel="stylesheet" href="/blog/css/gitment.css">
<link rel="stylesheet" href="/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/.css" src="/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/.js">

    <!--<link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">-->
    <link href="//at.alicdn.com/t/font_620856_pl6z7sid89qkt9.css" rel="stylesheet" type="text/css">
    <!-- ga & ba script hoook -->
    <script></script>
<meta name="generator" content="Hexo 5.4.0"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>

<body>

<!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="site-nav-toggle" id="site-nav-toggle">
    <button>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
    </button>
</div>

<div class="index-about">
    <i> 浙江大学可视分析小组博客 </i>
</div>

<div class="index-container">
    
    <div class="index-left">
        <div class="nav" id="nav">
    <div class="avatar-name">
        <div class="avatar ">
            <a href="/"><img
                    src="/img/avatar.png" /></a>
        </div>
        <div class="name">
            <a href="/">
                <i>ZJU VAI</i>
            </a>
        </div>
    </div>
    <div class="contents" id="nav-content">
        <ul>
            <li >
                <a
                    href="/">
                    <!-- <a href="/blog/about/"> -->
                    <!-- <i class="iconfont icon-fanhui"></i> -->
                    <span>🏠 小组主页</span>
                </a>
            </li>
            <li >
                <a href="/blog/">
                    <!-- <i class="iconfont icon-shouye1"></i> -->
                    <span>⌨️ 小组博客</span>
                </a>
            </li>
            <li >
                <a href="/blog/tags">
                    <!-- <i class="iconfont icon-biaoqian1"></i> -->
                    <span>📌 博客标签</span>
                </a>
            </li>
            <li >
                <a href="/blog/author">
                    <!-- <i class="iconfont icon-guanyu2"></i> -->
                    <span>👨‍🎓 作者存档</span>
                </a>
            </li>
            <li >
                <a href="/blog/archives">
                    <!-- <i class="iconfont icon-guidang2"></i> -->
                    <span>📅 时间存档</span>
                </a>
            </li>

            <li >
                <a href="/blog/textbook">
                    <!-- <i class="iconfont icon-biaoqian1"></i> -->
                    <span>📚 教材下载</span>
                </a>
            </li>
            
            <li>
                <a id="search">
                    <!-- <i class="iconfont icon-sousuo1"></i> -->
                    <span>🔎 博客搜索</span>
                </a>
            </li>
            

            <br />
            
            <li >
                <a target="_blank" rel="noopener" href="https://zjuvag.gitee.io/blog/">
                    <!-- <i class="iconfont icon-biaoqian2"></i> -->
                    <!-- <span style="color:#536589;font-weight:bold;">nav.mirror</span> -->
                </a>
            </li>
        </ul>
    </div>
    
    <div id="toc" class="toc-article">
    <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#Markov-Process"><span class="toc-text">Markov Process</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Definition"><span class="toc-text">Definition</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Markov-Reward-Process"><span class="toc-text">Markov Reward Process</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#G-t"><span class="toc-text">$G_t$</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0-Value-Function"><span class="toc-text">价值函数(Value Function)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%96%B9%E7%A8%8B-Bellman-equation"><span class="toc-text">贝尔曼方程(Bellman equation)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%9F%A9%E9%98%B5%E6%B1%82%E8%A7%A3"><span class="toc-text">矩阵求解</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Markov-Decision-Process"><span class="toc-text">Markov Decision Process</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AD%96%E7%95%A5-policy"><span class="toc-text">策略(policy)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0-2"><span class="toc-text">价值函数-2</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B0%86%E5%85%B6%E6%9C%80%E4%BC%98%E5%8C%96"><span class="toc-text">将其最优化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%80%E4%BC%98%E7%AD%96%E7%95%A5"><span class="toc-text">最优策略</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%80%E4%BC%98%E7%8A%B6%E6%80%81%E5%8A%A8%E4%BD%9C%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0"><span class="toc-text">最优状态动作价值函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E6%B1%82%E8%A7%A3"><span class="toc-text">如何求解</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#On-policy-vs-Off-policy"><span class="toc-text">On-policy vs Off-policy</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Policy"><span class="toc-text">Policy</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Policy-Gradient"><span class="toc-text">Policy Gradient</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#gt-policy-gradient-theorem"><span class="toc-text">&#x3D;&#x3D;&gt;policy gradient theorem</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#gt-%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%95%99%E6%8E%88-PPT"><span class="toc-text">&#x3D;&#x3D;&gt;李宏毅教授 PPT</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Actor-Critic"><span class="toc-text">Actor-Critic</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Reference"><span class="toc-text">Reference</span></a></li></ol>
</div>
    
</div>


<div class="search-field" id="search-field">
    <div class="search-container">
        <div class="search-input">
            <span id="esc-search"> <i class="icon-fanhui iconfont"></i></span>
            <input id="search-input" />
            <span id="begin-search">搜索</span>
        </div>
        <div class="search-result-container" id="search-result-container">

        </div>
    </div>
</div>
        <div class="index-about-mobile">
            <i> 浙江大学可视分析小组博客 </i>
        </div>
    </div>
    
    <div class="index-middle">
        <!-- Main Content -->
        <div class="post-container">
    <div class="post-title">
        Introduction of Reinforcement learning
    </div>

    <div class="post-meta">
        <!-- <span
            class="attr">发布于：<span>2019-09-06 14:33:32</span></span> -->
        <span class="attr">发布于：<span>2019-09-06</span></span>
        <span class="attr"><a class="tag" href="/blog/author/#陈则衔"
                title="陈则衔">陈则衔</a></span>
        
        <span class="attr">/
            
            <a class="tag" href="/blog/tags/#报告" title="报告">报告</a>
            <span>/</span>
            
            <a class="tag" href="/blog/tags/#论文评述" title="论文评述">论文评述</a>
            <span>/</span>
            
            
        </span>
        <span class="attr">访问：<span id="busuanzi_value_page_pv"></span>
        </span>
        </span>
    </div>
    <div class="post-content ">
        <h3 id="Markov-Process"><a href="#Markov-Process" class="headerlink" title="Markov Process"></a>Markov Process</h3><h4 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h4><ul>
<li>元组 $(\mathcal{S,P})$</li>
<li>$\mathcal{S}$是一个有限状态的集合</li>
<li>$\mathcal{P}$是一个状态转移矩阵：$\mathcal{P_{ss’}}=\mathbb{P}[\mathcal{S_t+1}=s’|\mathcal{S_t}=s]$</li>
</ul>
<h3 id="Markov-Reward-Process"><a href="#Markov-Reward-Process" class="headerlink" title="Markov Reward Process"></a>Markov Reward Process</h3><ul>
<li>在前者的基础上，增加了$(\mathcal{S,P,\color{red}{\mathcal{R,\gamma}}})$</li>
<li>$\color{red}{\mathcal{R}是一个奖励函数，\mathcal{R_s}=\mathbb{E}[R_{t+1}|\mathcal{S}=s]}$</li>
<li>$\color{red}\gamma是一个衰减因子，\gamma\in[0,1]$</li>
</ul>
<p>奖励函数$\mathcal{R}$代表了从状态$s$转移到状态$s’$时获得的奖励，这里奖励是离开状态后得到的(至于离开得到奖励还是进入一个新状态得到奖励只是定义了一种获得的规则而已)</p>
<h4 id="G-t"><a href="#G-t" class="headerlink" title="$G_t$"></a>$G_t$</h4><script type="math/tex; mode=display">
G_t=R_{t+1}+\gamma R_{t+2}+...=\mathcal{\sum_{k=0}^{\gamma^k}}R_{t+k+1}</script><p>代表了从状态$s$到最后状态$s_t$，得到的最终奖励，加入$\gamma$是因为距离越远，影响越小。即某一个具体 episode 所获得的 return。</p>
<p>目标是将其最大化。</p>
<h4 id="价值函数-Value-Function"><a href="#价值函数-Value-Function" class="headerlink" title="价值函数(Value Function)"></a>价值函数(Value Function)</h4><script type="math/tex; mode=display">
v(s)=\Bbb{E}[G_t|\mathcal{S_t}=s]</script><p>其代表着在状态$s$下的，$G_t$的期望值，因为从一个状态 s 出发有很多种不同的决策路径，得到不同的$G_t$</p>
<h4 id="贝尔曼方程-Bellman-equation"><a href="#贝尔曼方程-Bellman-equation" class="headerlink" title="贝尔曼方程(Bellman equation)"></a>贝尔曼方程(<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Bellman_equation">Bellman equation</a>)</h4><p><img src="http://www.cad.zju.edu.cn/home/vagblog/images/photo_bed/2019/9/23/26e48db0806441779d67fb70d06149ceb13d5f57.jpeg" alt=""></p>
<p>最后一行理由为：x 的期望的期望是 x 期望其本身.得到了一个重要的<strong>递归</strong>公式:</p>
<script type="math/tex; mode=display">
\begin{align}
v(s) &= \Bbb{E}[R_{t+1}+\gamma v(\mathcal{S_{t+1}})|\mathcal{S_t}=s] \\
&=\mathcal{R_s}+\gamma \sum_{s'\in S}\mathcal{P}_{ss'}v(s')
\end{align}</script><p>其有两部分组成，及时奖励的期望$\boldsymbol{R_{t+1}}$ ,下一个时刻状态$\boldsymbol{s_{t+1}}$ 的期望</p>
<h4 id="矩阵求解"><a href="#矩阵求解" class="headerlink" title="矩阵求解"></a>矩阵求解</h4><p><img src="http://www.cad.zju.edu.cn/home/vagblog/images/photo_bed/2019/9/23/0e260df4c3dcb2edacb8aa623869cbbf38489bc5.png" alt=""></p>
<h3 id="Markov-Decision-Process"><a href="#Markov-Decision-Process" class="headerlink" title="Markov Decision Process"></a>Markov Decision Process</h3><p><img src="http://www.cad.zju.edu.cn/home/vagblog/images/photo_bed/2019/9/23/b63cb3ae94de7ee1b2def12376e148cc76b44eb9.png" alt=""></p>
<h3 id="策略-policy"><a href="#策略-policy" class="headerlink" title="策略(policy)"></a>策略(policy)</h3><p><img src="http://www.cad.zju.edu.cn/home/vagblog/images/photo_bed/2019/9/23/e60ff0160a4c1bf5dd56d7da59ff14744cdf2d4c.png" alt=""></p>
<p>策略代表了在给定状态$s$下，可能的动作概率分布。</p>
<p><img src="http://www.cad.zju.edu.cn/home/vagblog/images/photo_bed/2019/9/23/ea03b301ad52dabdb1b6a3d3d574f52a8ade00bf.png" alt=""></p>
<h3 id="价值函数-2"><a href="#价值函数-2" class="headerlink" title="价值函数-2"></a>价值函数-2</h3><p><img src="http://www.cad.zju.edu.cn/home/vagblog/images/photo_bed/2019/9/23/cebfbeac0473b101e8e2b6961a3956c5561cfcb2.png" alt=""></p>
<script type="math/tex; mode=display">
\begin{align}
v_\pi(s)&=\sum_{a\in\mathcal{A}}\pi(a|s)q_\pi(s,a) \\
q_\pi(s,a)&=\mathcal{R}_s^a+\gamma\sum_{s'\in\mathcal{S}}\mathcal{P}_{ss'}^av_\pi(s')
\end{align}</script><p>===&gt;</p>
<script type="math/tex; mode=display">
\begin{align}
v_\pi(s)&=\sum_{a\in\mathcal{A}}\pi(a|s)(\mathcal{R}_s^a+\gamma\sum_{s'\in\mathcal{S}}\mathcal{P}_{ss'}^av_\pi(s'))\\
q_\pi(s,a)&=\mathcal{R}_s^a+\gamma\sum_{s'\in\mathcal{S}}\mathcal{P}_{ss'}^a\sum_{a\in\mathcal{A}}\pi(a'|s')q_\pi(s',a')
\end{align}</script><p>可以发现，也是个递归地过程</p>
<p>$v_\pi(s)$是由当前状态$s$下，策略$\pi$可能的动作概率*该动作下得到的奖励值，累加而成</p>
<p>$q_\pi(s,a)$由两部分组成，及时回报和执行这个操作后可能到达所有状态$s’$的价值函数的累加</p>
<h3 id="将其最优化"><a href="#将其最优化" class="headerlink" title="将其最优化"></a>将其最优化</h3><p><img src="http://www.cad.zju.edu.cn/home/vagblog/images/photo_bed/2019/9/23/fcfed2f0d035736b586dea4629cd4495a84cb73f.png" alt=""></p>
<h3 id="最优策略"><a href="#最优策略" class="headerlink" title="最优策略"></a>最优策略</h3><p><img src="http://www.cad.zju.edu.cn/home/vagblog/images/photo_bed/2019/9/23/271d2386628b35d246a88720d96f92399a5f2aa5.png" alt=""></p>
<h3 id="最优状态动作价值函数"><a href="#最优状态动作价值函数" class="headerlink" title="最优状态动作价值函数"></a>最优状态动作价值函数</h3><p><img src="http://www.cad.zju.edu.cn/home/vagblog/images/photo_bed/2019/9/23/9285e8b93a21b00c61499323dfa866000fa725f1.png" alt=""></p>
<p>彼此带入：</p>
<p><img src="http://www.cad.zju.edu.cn/home/vagblog/images/photo_bed/2019/9/23/0a36903192cf4db9b154eee65dbf817598ac8eb0.png" alt=""></p>
<h3 id="如何求解"><a href="#如何求解" class="headerlink" title="如何求解"></a>如何求解</h3><p>得到最优解的递归形式，如何求解就很关键了。主要方法有：value Function(Q-learning,Sarsa);Policy gradient(PPO);AC 等等.</p>
<p><img src="http://www.cad.zju.edu.cn/home/vagblog/images/photo_bed/2019/9/23/fd34e0959d91c416d55eba2a406924d9f09f1a1d.png" alt=""></p>
<p>_Fig. Summary of approaches in RL based on whether we want to model the value, policy, or the environment. (Image source: reproduced from David Silver’s RL course lecture 1.)_</p>
<h3 id="On-policy-vs-Off-policy"><a href="#On-policy-vs-Off-policy" class="headerlink" title="On-policy vs Off-policy"></a>On-policy vs Off-policy</h3><ul>
<li><strong>Model-based</strong>: Rely on the model of the environment; either the model is known or the algorithm learns it explicitly.(_When we fully know the environment, we can find the optimal solution by <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Dynamic_programming">Dynamic Programming</a> (DP)._)</li>
<li><strong>Model-free</strong>: No dependency on the model during learning.</li>
<li><strong>Model-based</strong>尝试着 model 整个环境；先 model 了这个环境，基于该环境做出最优的策略；<strong>Model-free</strong>就是走一步看一步，在每一步中去尝试学习最优的策略。</li>
<li><p>_The model-based learning uses environment, action and reward to get the most reward from the action. The model-free learning only uses its action and reward to infer the best action._</p>
</li>
<li><p><strong>On-policy</strong>: The agent learned and the agent interacting with the environment is the same.(<strong>自己和环境互动</strong>)</p>
</li>
<li><strong>Off-policy</strong>:The agent learned and the agent interacting with the environment is different.(<strong>自己看别人玩</strong>)</li>
</ul>
<hr>
<h3 id="Policy"><a href="#Policy" class="headerlink" title="Policy"></a>Policy</h3><p>Policy $\pi$,代表在状态$s$，会执行的动作$a$.(分为确定性和随机性)</p>
<ul>
<li>Deterministic: $\pi(s)=a.$</li>
<li>Stochastic: $\pi(a|s)=\mathbb{P}_\pi[A=a|S=s]$</li>
<li>$\pi_\theta(a|s)=\mathbb{P}_\pi[A=a|S=s,\theta]$</li>
</ul>
<h4 id="Policy-Gradient"><a href="#Policy-Gradient" class="headerlink" title="Policy Gradient"></a>Policy Gradient</h4><p><strong>object function</strong>:</p>
<p><strong>episodic environments:</strong></p>
<p>$J_1(\theta)=V^{\pi_\theta}(s_1)=\mathbb{E}_{\pi_\theta}[V_1]$</p>
<p><strong>continuing environments:</strong></p>
<p>_连续型环境就没有初始状态一说了，那么就取平均，考虑 agent 在某时刻处于某个状态下的概率，即状态分布_</p>
<p><strong>average value:</strong></p>
<p>$J_{avV}(\theta)=\sum_sd^{\pi_\theta}(s)V^{\pi_\theta}(s)$</p>
<p>对每个可能的状态计算从该时刻(该状态)开始于环境一直交互下去的奖励，然后对其概率分布求和.</p>
<p><strong>average reward per time-step:</strong></p>
<p>$J_{avR}(\theta)=\sum_sd^{\pi_\theta}(s)\sum_a\pi_\theta(s,a)\mathcal{R}_s^a$</p>
<p>其中,$d^{\pi_\theta}(s)$是马尔科夫链在$\pi_\theta$下的随机概率分布；即$d^\pi(s)=lim_{t→∞}P(s_t=s|s_0,\pi_θ)$就表示你从状态$s_0$开始，在策略$\pi_θ$下经过$t$个时间步后到达状态$s$的概率。</p>
<p>该式代表了到达某个状态$s$,并且采用动作$a$的情况下，获得的平均回报$\mathcal{R_s^a}$,概率化累加就是平均回报.</p>
<p>这个可以如下计算(以李宏毅教授 PPT 为例)，每条迹的概率*该条迹累计的期望即为平均</p>
<p><img src="http://www.cad.zju.edu.cn/home/vagblog/images/photo_bed/2019/9/23/2d99f29215e2162298d27f45456142a4ed46ab00.png" alt=""></p>
<p>目标是将目标函数最大化，可以用梯度下降法将其最大化</p>
<p>_注：用基于梯度的策略优化时，是基于序列结构片段，如果无穷地和环境交互，得到一个结果是没有意义的；就是和环境交互中的某一个序列，将其拿出来进行学习，然后优化策略_</p>
<p><strong>one-step MDP(per time-step):</strong></p>
<script type="math/tex; mode=display">
\begin{split}
J(\theta)&=\sum_sd^{\pi_\theta}(s)\sum_a\pi_\theta(s,a)\mathcal{R}_s^a
\\
\nabla_\theta J(\theta)&=\sum_sd^{\pi_\theta}(s)\sum_a\pi_\theta(s,a)\nabla_\theta log\pi_\theta(s,a)\mathcal{R}_s^a
\end{split}</script><h4 id="gt-policy-gradient-theorem"><a href="#gt-policy-gradient-theorem" class="headerlink" title="==&gt;policy gradient theorem"></a>==&gt;policy gradient theorem</h4><p><img src="http://www.cad.zju.edu.cn/home/vagblog/images/photo_bed/2019/9/23/3741b1b76e122866288c4b7ee69add9749fd1988.png" alt=""></p>
<h4 id="gt-李宏毅教授-PPT"><a href="#gt-李宏毅教授-PPT" class="headerlink" title="==&gt;李宏毅教授 PPT"></a>==&gt;李宏毅教授 PPT</h4><p><img src="http://www.cad.zju.edu.cn/home/vagblog/images/photo_bed/2019/9/23/74a31be33bfec962a642414a023273ca7f0567c1.png" alt="AC-3"></p>
<p><br></p>
<h3 id="Actor-Critic"><a href="#Actor-Critic" class="headerlink" title="Actor-Critic"></a>Actor-Critic</h3><p>AC 是在基于 Policy Gradient 的基础上进行了变化，融合了 Value based 的方法.</p>
<p>Actor:就是把$J(\theta)$最大化，但是梯度里面的$Q^{\pi_\theta}(s,a)$用 Value based 的方法来计算，结果如下：</p>
<p><img src="http://www.cad.zju.edu.cn/home/vagblog/images/photo_bed/2019/9/23/85df5905cf5e757bfb93f10f49b0b7908d9a440b.png" alt="AC-4"></p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/liweibin1994/article/details/79079884">csdn-blog</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/28084942">David Silver 强化学习公开课</a></p>
<p>David Silver slides</p>
<p>博客:</p>
<p><a target="_blank" rel="noopener" href="https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#deadly-triad-issue">https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#deadly-triad-issue</a></p>
<p><a target="_blank" rel="noopener" href="https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html">https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html</a></p>
<p>李宏毅教授：</p>
<p>David_Silver:</p>
<p>配套 PPT</p>
<p>Mnih, Volodymyr, et al. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1602.01783">“Asynchronous methods for deep reinforcement learning.”</a> ICML. 2016.</p>
<p>David Silver, et al. <a target="_blank" rel="noopener" href="https://hal.inria.fr/file/index/docid/938992/filename/dpg-icml2014.pdf">“Deterministic policy gradient algorithms.”</a> ICML. 2014.</p>
<p>Timothy P. Lillicrap, et al. <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1509.02971.pdf">“Continuous control with deep reinforcement learning.”</a> arXiv preprint arXiv:1509.02971 (2015).</p>

        
        <br />
        <div id="comment-container">
        </div>
        <div id="disqus_thread"></div>
        <hr/>
        <div id="lv-container">
            Questions & Discussion：<a href="mailto:zexianchen@zju.edu.cn"><code> ✉️ zexianchen@zju.edu.cn </code> </a>
        </div>
        <hr/>
    </div>
</div>
    </div>
</div>

<footer class="footer">
    <ul class="list-inline text-center">
             
    </ul>
    
    <p>
        Created By <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a> Theme
        <a target="_blank" rel="noopener" href="https://github.com/aircloud/hexo-theme-aircloud">AirCloud</a>
    </p>
</footer>


<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdn.bootcdn.net/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<!-- <script src="/blog/js/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script> --><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>

<script>
    // We expose some of the variables needed by the front end
    window.hexo_search_path = "search.json"
    window.hexo_root = "/blog/"
    window.isPost = true
</script>
<script src="https://cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script>

<script src="/blog/js/index.js"></script>
<script href="/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/.css" src="/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/blog/.js"></script>

<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




</html>
